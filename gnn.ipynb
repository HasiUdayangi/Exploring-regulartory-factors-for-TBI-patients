{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a0f0d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML, clear_output\n",
    "import sys\n",
    "import boto3\n",
    "import io\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime as dt\n",
    "from datetime import datetime, timedelta, date\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch_geometric.nn import GCNConv\n",
    "import networkx as nx\n",
    "from torch_geometric.data import Data, DataLoader, Dataset\n",
    "\n",
    "wh = Waveform_Helper()\n",
    "\n",
    "athena = Athena_Query()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb675de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load and preprocess data\n",
    "# Merge dataframes\n",
    "merged_df = hrv.merge(vital, on='patientid', how='inner')\n",
    "merged_df = merged_df.merge(lab, on='patientid', how='inner')\n",
    "merged_df = merged_df.merge(gcs, on='patientid', how='inner')\n",
    "merged_df = merged_df.merge(age, on='patientid', how='inner')\n",
    "metrics = ['SDNN (ms)', 'SDANN (ms)', 'MeanRR (ms)', 'RMSSD (ms)', 'pNN50 (%)', 'P_VLF (%)','P_LF (%)','P_HF (%)','HF/LF', 'REC (%)','DET (%)','LAM (%)','SD1', 'SD2', 'alpha1' ,'alpha2',\n",
    "          'HR (mean)','Diastolic BP (mean)','Systolic BP (mean)','ABP Mean (mean)','SpO2 (mean)','HR (slope)','Diastolic BP (slope)','Systolic BP (slope)','ABP Mean (slope)','SpO2 (slope)','HR_std','Diastolic BP_std','Systolic BP_std','ABP Mean_std','SpO2_std',\n",
    "          'so2','po2','pco2','hemoglobin','carboxyhemoglobin', 'methemoglobin','chloride','calcium','anion_gap','temperature','potassium','sodium','lactate','glucose',\n",
    "          'GCS(eyes)_12hr','GCS(motor)_12hr','GCS(verbal)_12hr','GCS_final',\n",
    "          'age_on_adm','gender']\n",
    "\n",
    "# Calculate correlation matrix\n",
    "correlation_matrix = merged_df[metrics].corr()\n",
    "plt.figure(figsize=(20, 20))\n",
    "# Visualize correlation matrix as heatmap\n",
    "sns.heatmap(correlation_matrix, annot=False, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "\n",
    "\n",
    "group_boundaries = [0, 16, 31, 45, 49, 51]\n",
    "\n",
    "# Add group separation lines\n",
    "for boundary in group_boundaries:\n",
    "    plt.axhline(y=boundary, color='r', linewidth=2)\n",
    "    plt.axvline(x=boundary, color='r', linewidth=2)\n",
    "    \n",
    "    \n",
    "plt.savefig(f'heatmap.png', dpi=100, bbox_inches='tight')\n",
    "plt.title(\"Feature Correlation Heatmap\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9d84a168",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "66fbc5c4",
   "metadata": {},
   "source": [
    "***New method***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b07586",
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.Graph()\n",
    "\n",
    "metrics = ['SDNN (ms)', 'SDANN (ms)', 'MeanRR (ms)', 'RMSSD (ms)', 'pNN50 (%)', 'P_VLF (%)','P_LF (%)','P_HF (%)','HF/LF', 'REC (%)','DET (%)','LAM (%)','SD1', 'SD2', 'alpha1' ,'alpha2',\n",
    "          'HR (mean)','Diastolic BP (mean)','Systolic BP (mean)','ABP Mean (mean)','SpO2 (mean)','HR (slope)','Diastolic BP (slope)','Systolic BP (slope)','ABP Mean (slope)','SpO2 (slope)','HR_std','Diastolic BP_std','Systolic BP_std','ABP Mean_std','SpO2_std',\n",
    "          'so2','po2','pco2','hemoglobin','carboxyhemoglobin', 'methemoglobin','chloride','calcium','anion_gap','temperature','potassium','sodium','lactate','glucose',\n",
    "          'GCS(eyes)_12hr','GCS(motor)_12hr','GCS(verbal)_12hr','GCS_final',\n",
    "          'age_on_adm','gender']\n",
    "# Define the subsets and their colors\n",
    "set_colors = {'hrv': 'blue', 'vital': 'green', 'lab': 'red', 'gcs': 'gray', 'age': 'yellow'}\n",
    "\n",
    "# Initialize a dictionary to store community assignments for each subset\n",
    "community_assignments = {subset: {} for subset in set_colors.keys()}\n",
    "\n",
    "# Add nodes (features) to the graph and assign colors\n",
    "for feature in merged_[metrics]:\n",
    "    subset = feature_sets.loc[feature.strip()]['Set']\n",
    "    G.add_node(feature, color=set_colors[subset])\n",
    "\n",
    "    \n",
    "\n",
    "correlation_matrix = merged_[metrics].corr()\n",
    "# Add edges (significant correlations) with a weight of 1\n",
    "threshold = 0.5  # Adjust this threshold as needed\n",
    "for i in range(len(correlation_matrix.columns)):\n",
    "    for j in range(i + 1, len(correlation_matrix.columns)):\n",
    "        if abs(correlation_matrix.iloc[i, j]) > threshold:\n",
    "            G.add_edge(correlation_matrix.columns[i], correlation_matrix.columns[j], weight=1)\n",
    "\n",
    "# Create a mapping from node names to integers\n",
    "node_to_index = {node: i for i, node in enumerate(G.nodes)}\n",
    "\n",
    "# Map the nodes to integers in the edge list\n",
    "edge_index = torch.tensor([(node_to_index[u], node_to_index[v]) for u, v in G.edges], dtype=torch.long).t().contiguous()\n",
    "\n",
    "\n",
    "# Convert the adjacency matrix to a dense PyTorch tensor\n",
    "adjacency_matrix = nx.adjacency_matrix(G)\n",
    "adjacency_matrix = torch.tensor(adjacency_matrix.todense(), dtype=torch.float)\n",
    "\n",
    "# Print adjacency matrix and edge indices\n",
    "print(\"Edge Index Shape:\", edge_index.shape)\n",
    "print(\"Edge Index Content:\", edge_index)\n",
    "print(\"Adjacency Matrix:\")\n",
    "print(adjacency_matrix)\n",
    "print(\"Edge Indices:\")\n",
    "print(edge_index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d83ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Compute the layout positions of the nodes using a force-directed layout algorithm\n",
    "pos = nx.spring_layout(G, seed=42)  # You can choose different layout algorithms as well\n",
    "\n",
    "# Define node colors based on subsets\n",
    "node_colors = [node[1]['color'] for node in G.nodes(data=True)]\n",
    "plt.figure(figsize=(20, 20))\n",
    "# Draw nodes and edges\n",
    "nx.draw_networkx_nodes(G, pos, node_color=node_colors, node_size=200, cmap=plt.cm.Blues)\n",
    "nx.draw_networkx_edges(G, pos, width=1.0, alpha=0.5)\n",
    "\n",
    "# Draw node labels if needed\n",
    "nx.draw_networkx_labels(G, pos, font_size=8, font_color='black', font_weight='bold')\n",
    "plt.savefig(\"gnn_grapg.png\") \n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6592be63",
   "metadata": {},
   "source": [
    "***AUTOENCODER METHOD***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e484257",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, adjacency_matrix, edge_index):\n",
    "        self.graph_data = Data(x=torch.tensor(adjacency_matrix, dtype=torch.float),\n",
    "                               edge_index=edge_index)\n",
    "\n",
    "    def __len__(self):\n",
    "        return 1  # This dataset contains only one graph\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.graph_data\n",
    "\n",
    "# Create a DataLoader for your dataset\n",
    "dataset = CustomDataset(adjacency_matrix, edge_index)\n",
    "train_loader = DataLoader(dataset, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "04539e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_collate(batch):\n",
    "    # `batch` is a list of Data objects\n",
    "    return torch_geometric.data.Batch.from_data_list(batch)\n",
    "\n",
    "# Create a DataLoader with the custom collate function\n",
    "train_loader = DataLoader(dataset, batch_size=1, shuffle=True, collate_fn=custom_collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ca4e4a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the GCN model\n",
    "import torch.nn as nn\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "class GraphAutoencoder(nn.Module):\n",
    "    def __init__(self, num_features, hidden_dim):\n",
    "        super(GraphAutoencoder, self).__init__()\n",
    "        self.encoder = GCNConv(num_features, hidden_dim)\n",
    "        self.decoder = GCNConv(hidden_dim, num_features)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        # Encoder\n",
    "        x = self.encoder(x, edge_index)\n",
    "        # Decoder\n",
    "        x = self.decoder(x, edge_index)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed2a355f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch_geometric\n",
    "from torch_geometric.data import Data\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "from torch_geometric.nn import GCNConv\n",
    "import networkx as nx\n",
    "\n",
    "num_features = adjacency_matrix.shape[0]  # Number of nodes in the graph\n",
    "hidden_dim = 64\n",
    "model = GraphAutoencoder(num_features, hidden_dim)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training Loop\n",
    "num_epochs = 1000  # Adjust the number of epochs as needed\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = GraphAutoencoder(num_features, hidden_dim).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        data = batch.to(device)  # Send the data to the appropriate device (CPU or GPU)\n",
    "        output = model(data.x, data.edge_index)\n",
    "        loss = torch.nn.functional.mse_loss(output, data.x)  # Mean Squared Error as the reconstruction loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    average_loss = total_loss / len(train_loader)\n",
    "    print(f'Epoch: {epoch + 1}, Loss: {average_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "163c05c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have a graph G created using networkx\n",
    "\n",
    "# Step 1: Compute node degrees\n",
    "node_degrees = dict(G.degree())\n",
    "\n",
    "# Step 2: Sort nodes based on degrees in descending order\n",
    "sorted_nodes = sorted(node_degrees, key=node_degrees.get, reverse=True)\n",
    "\n",
    "# Step 3: Get top 20 nodes\n",
    "top_20_nodes = sorted_nodes[:20]\n",
    "\n",
    "# Print or use the top 20 nodes as needed\n",
    "print(\"Top 20 Nodes based on Degree:\", top_20_nodes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dab578d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from node2vec import Node2Vec\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "node2vec = Node2Vec(G, dimensions=64, walk_length=30, num_walks=200, workers=4)\n",
    "\n",
    "# Fit the model (generate node embeddings)\n",
    "model = node2vec.fit(window=10, min_count=1, batch_words=4)\n",
    "\n",
    "# Get the embeddings for all nodes\n",
    "node_embeddings = model.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5fbe28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce dimensionality with PCA\n",
    "pca = PCA(n_components=2)\n",
    "node_embeddings_2d = pca.fit_transform(node_embeddings.vectors)\n",
    "\n",
    "# Extract node names (replace with your node labels or IDs)\n",
    "node_names = list(G.nodes())\n",
    "\n",
    "# Create a scatter plot for visualization\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(node_embeddings_2d[:, 0], node_embeddings_2d[:, 1], c='b', marker='o')\n",
    "\n",
    "# Annotate nodes with their labels or IDs\n",
    "for i, txt in enumerate(node_names):\n",
    "    plt.annotate(txt, (node_embeddings_2d[i, 0], node_embeddings_2d[i, 1]))\n",
    "\n",
    "plt.title('Node Embedding Visualization (2D)')\n",
    "plt.xlabel('Dimension 1')\n",
    "plt.ylabel('Dimension 2')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c82d94",
   "metadata": {},
   "source": [
    "***After apply hyperparametwr tuning***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c3292404",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'hidden_dim': 256, 'learning_rate': 0.01}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.data import Data\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, num_features, hidden_dim, num_classes):\n",
    "        super(GCN, self).__init__()\n",
    "        self.conv1 = GCNConv(num_features, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, num_classes)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = torch.relu(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return 1  # This dataset contains only one graph\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data\n",
    "\n",
    "def train(model, data, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    output = model(data.x, data.edge_index)\n",
    "    loss = criterion(output, data.x)  # Mean Squared Error as the reconstruction loss\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "def evaluate(model, data, criterion, device):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        output = model(data.x, data.edge_index)\n",
    "        loss = criterion(output, data.x)\n",
    "    return loss.item()\n",
    "\n",
    "# Define your hyperparameters grid to search\n",
    "hidden_dims = [32, 64, 128, 256]\n",
    "learning_rates = [0.001, 0.01, 0.1]\n",
    "num_epochs = 1000\n",
    "num_classes = 51\n",
    "\n",
    "# Define your dataset and DataLoader here (assuming you have 'dataset' and 'train_loader')\n",
    "\n",
    "best_loss = float('inf')\n",
    "best_params = None\n",
    "\n",
    "# Grid search for hyperparameter tuning\n",
    "for hidden_dim in hidden_dims:\n",
    "    for learning_rate in learning_rates:\n",
    "        model = GCN(num_features, hidden_dim, num_classes).to(device)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "        criterion = nn.MSELoss()  # Mean Squared Error Loss\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            train_loss = train(model, data, optimizer, criterion, device)\n",
    "            val_loss = evaluate(model, data, criterion, device)\n",
    "            \n",
    "            # Print training progress if needed\n",
    "            # print(f'Epoch: {epoch + 1}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')\n",
    "\n",
    "        if val_loss < best_loss:\n",
    "            best_loss = val_loss\n",
    "            best_params = {'hidden_dim': hidden_dim, 'learning_rate': learning_rate}\n",
    "\n",
    "print(\"Best Hyperparameters:\", best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f320f741",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your GCN model with the best hyperparameters\n",
    "best_hidden_dim = 256\n",
    "best_learning_rate = 0.01\n",
    "\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, num_features, hidden_dim, num_classes):\n",
    "        super(GCN, self).__init__()\n",
    "        self.conv1 = GCNConv(num_features, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, num_classes)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = torch.relu(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x\n",
    "\n",
    "# Instantiate the model, send it to the appropriate device, and set up the optimizer\n",
    "model = GCN(num_features, best_hidden_dim, num_classes).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=best_learning_rate)\n",
    "\n",
    "# Train the model using your full dataset (data) and its edge indices (edge_index) for a certain number of epochs\n",
    "num_epochs = 10000  # You can adjust the number of epochs based on your dataset and model complexity\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    optimizer.zero_grad()\n",
    "    output = model(data.x, data.edge_index)\n",
    "    loss = torch.nn.functional.mse_loss(output, data.x)  # Mean Squared Error as the reconstruction loss\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# Get node embeddings after training\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    node_embeddings = model(data.x, data.edge_index)\n",
    "\n",
    "# Assuming you want to select the top nodes based on L2 norm of embeddings\n",
    "node_norms = torch.norm(node_embeddings, dim=1)\n",
    "_, indices = torch.topk(node_norms, k=20, largest=True)\n",
    "top_20_nodes = list(indices.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "444b89c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Calculate L2 norms for node embeddings\n",
    "node_norms = torch.norm(node_embeddings, dim=1)\n",
    "\n",
    "\n",
    "# Select top 20 features based on L2 norms\n",
    "_, indices = torch.topk(node_norms, k=20, largest=True)\n",
    "top_20_nodes = [i.item() for i in indices]\n",
    "\n",
    "# Map the indices back to feature names based on your graph's node labels\n",
    "feature_names = [list(G.nodes())[i] for i in top_20_nodes]\n",
    "\n",
    "# Get top 20 scores for the selected features (nodes)\n",
    "top_20_scores = [node_norms[i].item() for i in top_20_nodes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b712a4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'GCS_FINAL' in feature_names:\n",
    "    feature_names.remove('GCS_FINAL')\n",
    "    top_20_scores = [score for feature, score in zip(feature_names, top_20_scores) if feature != 'GCS_FINAL']\n",
    "\n",
    "# Plot the bar chart for the top 20 features and their scores\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(np.arange(len(feature_names)), top_20_scores, align='center')\n",
    "plt.yticks(np.arange(len(feature_names)), feature_names)\n",
    "plt.xlabel('Importance Score')\n",
    "plt.ylabel('Node Attributes')\n",
    "plt.title('Top 20 important')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.savefig(f'feature importance GCN top 20_new.png', dpi=100, bbox_inches='tight')# Invert the y-axis to show the most important feature at the top\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb9f33f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Calculate L2 norms for node embeddings excluding GCS_final index\n",
    "node_norms = torch.norm(node_embeddings, dim=1)\n",
    "\n",
    "# Exclude GCS_final index from normalized_scores and feature_names\n",
    "normalized_scores = [(score - node_norms.min()) / (node_norms.max() - node_norms.min()) for i, score in enumerate(node_norms) if i != gcs_final_index]\n",
    "feature_names = [list(G.nodes())[i] for i in range(len(node_norms)) if i != gcs_final_index]\n",
    "\n",
    "# Create an array of angles for the radar chart\n",
    "angles = np.linspace(0, 2 * np.pi, len(feature_names), endpoint=False).tolist()\n",
    "\n",
    "# Duplicate the first angle and append it to match the extra normalized score\n",
    "angles += angles[:1]\n",
    "\n",
    "# Create a figure and axis\n",
    "plt.figure(figsize=(8, 8))\n",
    "ax = plt.subplot(111, polar=True)\n",
    "\n",
    "# Fill the radar plot\n",
    "ax.fill(angles, normalized_scores + [normalized_scores[0]], 'b', alpha=0.1)  # Append the first score to match the length\n",
    "\n",
    "# Set the labels for each axis\n",
    "ax.set_xticks(angles[:-1])\n",
    "ax.set_xticklabels(feature_names)\n",
    "\n",
    "# Set the y-labels and limit the y-axis to 0-1\n",
    "ax.set_rlabel_position(0)\n",
    "plt.yticks([0, 0.2, 0.4, 0.6, 0.8, 1.0], [\"0\", \"0.2\", \"0.4\", \"0.6\", \"0.8\", \"1.0\"], color=\"grey\", size=10)\n",
    "plt.ylim(0, 1)\n",
    "plt.title('Normalized Important Scores for All Features')\n",
    "plt.savefig(f'feature_importance_GCN_radar_chart.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09787950",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort features based on normalized scores in descending order\n",
    "sorted_indices = sorted(range(len(normalized_scores)), key=lambda k: normalized_scores[k], reverse=True)\n",
    "\n",
    "# Select top 20 features\n",
    "top_20_indices = sorted_indices[:20]\n",
    "top_20_feature_names = [feature_names[i] for i in top_20_indices]\n",
    "top_20_normalized_scores = [normalized_scores[i] for i in top_20_indices]\n",
    "\n",
    "# Create an array of angles for the radar chart\n",
    "angles = np.linspace(0, 2 * np.pi, len(top_20_feature_names), endpoint=False).tolist()\n",
    "angles += angles[:1]  # Duplicate the first angle to close the radar chart\n",
    "\n",
    "# Create a figure and axis\n",
    "plt.figure(figsize=(8, 8))\n",
    "ax = plt.subplot(111, polar=True)\n",
    "\n",
    "# Fill the radar plot for top 20 features\n",
    "ax.fill(angles, top_20_normalized_scores + [top_20_normalized_scores[0]], 'b', alpha=0.1)  \n",
    "\n",
    "# Set the labels for each axis\n",
    "ax.set_xticks(angles[:-1])\n",
    "ax.set_xticklabels(top_20_feature_names)\n",
    "\n",
    "# Set the y-labels and limit the y-axis to 0-1\n",
    "ax.set_rlabel_position(0)\n",
    "plt.yticks([0, 0.2, 0.4, 0.6, 0.8, 1.0], [\"0\", \"0.2\", \"0.4\", \"0.6\", \"0.8\", \"1.0\"], color=\"grey\", size=10)\n",
    "plt.ylim(0, 1)\n",
    "plt.title('Normalized Important Scores for Top 20 Features')\n",
    "plt.savefig(f'feature_importance_GCN_top_20_radar_chart.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6861a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "\n",
    "# Calculate importance scores for nodes in G_top_k\n",
    "top_k_scores = [node_norms[i].item() for i in indices]\n",
    "\n",
    "# Create a new graph containing only the top 20 nodes and their edges\n",
    "top_k_nodes = [list(G.nodes())[i] for i in indices]\n",
    "G_top_k = G.subgraph(top_k_nodes)\n",
    "\n",
    "# Define a color map based on importance scores using the 'coolwarm' colormap\n",
    "color_map = []\n",
    "for node in G_top_k.nodes():\n",
    "    if node in top_k_nodes:\n",
    "        index = top_k_nodes.index(node)\n",
    "        score = top_k_scores[index]\n",
    "        # Map the scores to colors using the 'coolwarm' colormap\n",
    "        color = plt.cm.coolwarm(score / max(top_k_scores))  # coolwarm colormap, normalized to the maximum score\n",
    "        color_map.append(color)\n",
    "    else:\n",
    "        color_map.append('lightgrey')  # Color for nodes not in the top 20\n",
    "\n",
    "# Draw the entire graph with all edges\n",
    "plt.figure(figsize=(25, 25))\n",
    "pos = nx.spring_layout(G, seed=42)\n",
    "\n",
    "# Draw all nodes with labels and colors based on real importance scores\n",
    "all_nodes = nx.draw_networkx_nodes(G, pos, node_color='lightgrey', node_size=100)\n",
    "all_edges = nx.draw_networkx_edges(G, pos, edge_color='lightgrey', alpha=0.3)\n",
    "\n",
    "# Draw top 20 nodes with labels and colors based on real importance scores\n",
    "top_nodes = nx.draw_networkx_nodes(G_top_k, pos, node_color=color_map, node_size=500)\n",
    "top_edges = nx.draw_networkx_edges(G_top_k, pos, edge_color='grey')\n",
    "\n",
    "# Annotate top 20 nodes with their labels and scores\n",
    "node_labels = {node: f\"{node}\\n{score:.2f}\" for node, score in zip(top_k_nodes, top_k_scores)}\n",
    "labels = nx.draw_networkx_labels(G_top_k, pos, labels=node_labels, font_size=10)\n",
    "\n",
    "# Remove node labels and axis\n",
    "plt.axis('off')\n",
    "plt.savefig(f'top 20 GRAPH_new.png')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44717c3a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
